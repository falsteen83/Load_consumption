{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d1ff98f-9083-4ee8-9e10-d965abae5984",
   "metadata": {},
   "source": [
    "# Joining consumotion and weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cded829f-6cf3-4398-80a2-ddc1be081a30",
   "metadata": {},
   "source": [
    "## Import the data from there files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d86f325d-7710-4b0b-b1c5-5b8aa62a8003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d4c9e55-376d-49dc-b328-c5c9c2748e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id                              name source        lon        lat  \\\n",
      "0        24  Queen Alia International Airport    JMD  36.019444  31.727222   \n",
      "1        24  Queen Alia International Airport    JMD  36.019444  31.727222   \n",
      "2        24  Queen Alia International Airport    JMD  36.019444  31.727222   \n",
      "3        24  Queen Alia International Airport    JMD  36.019444  31.727222   \n",
      "4        24  Queen Alia International Airport    JMD  36.019444  31.727222   \n",
      "...      ..                               ...    ...        ...        ...   \n",
      "8467900  29                       Ras Muneeef    JMD  35.811667  32.380278   \n",
      "8467901  29                       Ras Muneeef    JMD  35.811667  32.380278   \n",
      "8467902  29                       Ras Muneeef    JMD  35.811667  32.380278   \n",
      "8467903  29                       Ras Muneeef    JMD  35.811667  32.380278   \n",
      "8467904  29                       Ras Muneeef    JMD  35.811667  32.380278   \n",
      "\n",
      "            alt     id.1           date_time  horizontal_visibility_km  \\\n",
      "0         722.0  4381376 2021-05-25 00:00:00                       6.0   \n",
      "1         722.0  4381377 2021-05-25 03:00:00                       6.0   \n",
      "2         722.0  4381378 2021-05-25 06:00:00                       6.0   \n",
      "3         722.0  4385595 2021-06-11 09:00:00                       6.0   \n",
      "4         722.0  4381382 2021-05-25 18:00:00                       6.0   \n",
      "...         ...      ...                 ...                       ...   \n",
      "8467900  1150.0  3250342 2023-12-31 23:35:00                       NaN   \n",
      "8467901  1150.0  3250343 2023-12-31 23:40:00                       NaN   \n",
      "8467902  1150.0  3250344 2023-12-31 23:45:00                       NaN   \n",
      "8467903  1150.0  3250345 2023-12-31 23:50:00                       NaN   \n",
      "8467904  1150.0  3250346 2023-12-31 23:55:00                       NaN   \n",
      "\n",
      "         cloud_cover_oktas  wind_speed_knot  wind_direction_degree  \\\n",
      "0                      0.0              NaN                    NaN   \n",
      "1                      NaN              NaN                    NaN   \n",
      "2                      0.0              NaN                    NaN   \n",
      "3                      NaN              NaN                    NaN   \n",
      "4                      0.0              NaN                    NaN   \n",
      "...                    ...              ...                    ...   \n",
      "8467900                NaN            5.303                  164.1   \n",
      "8467901                NaN            5.027                  163.8   \n",
      "8467902                NaN            5.759                  160.6   \n",
      "8467903                NaN            6.118                  161.3   \n",
      "8467904                NaN            6.050                  160.0   \n",
      "\n",
      "         irradiance  relative_humidity_percent  air_temperature_c  \\\n",
      "0               NaN                        NaN                NaN   \n",
      "1               NaN                        NaN                NaN   \n",
      "2               NaN                        NaN                NaN   \n",
      "3               NaN                        NaN                NaN   \n",
      "4               NaN                        NaN                NaN   \n",
      "...             ...                        ...                ...   \n",
      "8467900         0.0                       90.7                8.0   \n",
      "8467901         0.0                       90.3                7.9   \n",
      "8467902         0.0                       90.2                8.0   \n",
      "8467903         0.0                       89.7                8.0   \n",
      "8467904         0.0                       90.0                8.0   \n",
      "\n",
      "         rainfall_mm  weather_source_id  pressure_mbar  air_density_c  \\\n",
      "0                NaN                 24            NaN            NaN   \n",
      "1                NaN                 24            NaN            NaN   \n",
      "2                NaN                 24            NaN            NaN   \n",
      "3                NaN                 24            NaN            NaN   \n",
      "4                NaN                 24            NaN            NaN   \n",
      "...              ...                ...            ...            ...   \n",
      "8467900          0.0                 29            NaN            NaN   \n",
      "8467901          0.0                 29            NaN            NaN   \n",
      "8467902          0.0                 29            NaN            NaN   \n",
      "8467903          0.0                 29            NaN            NaN   \n",
      "8467904          0.0                 29            NaN            NaN   \n",
      "\n",
      "         module_temp_c  \n",
      "0                  NaN  \n",
      "1                  NaN  \n",
      "2                  NaN  \n",
      "3                  NaN  \n",
      "4                  NaN  \n",
      "...                ...  \n",
      "8467900            NaN  \n",
      "8467901            NaN  \n",
      "8467902            NaN  \n",
      "8467903            NaN  \n",
      "8467904            NaN  \n",
      "\n",
      "[8467905 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get all CSV file paths\n",
    "#weather_csv_files = glob.glob('./weather_data/*.csv')\n",
    "# Get all CSV file paths\n",
    "weather_csv_path=r\"/Users/computer/Downloads/Load/Load/data/weather_data/*.csv\"\n",
    "weather_csv_files = glob.glob(weather_csv_path)\n",
    "\n",
    "# Read and combine all files into one DataFrame\n",
    "weather_df = [pd.read_csv(file, parse_dates=['date_time']) for file in weather_csv_files]\n",
    "\n",
    "# Merge dataframes of all processed files\n",
    "weather_df = pd.concat(weather_df, ignore_index=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(weather_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55943751-bb79-40d2-be7a-b7080a69d757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id                       name          x          y     id.1  \\\n",
      "0         3             Mwaqar (MWQAR)  36.127889  31.796571   171905   \n",
      "1         3             Mwaqar (MWQAR)  36.127889  31.796571   171906   \n",
      "2         3             Mwaqar (MWQAR)  36.127889  31.796571   171907   \n",
      "3         3             Mwaqar (MWQAR)  36.127889  31.796571   171908   \n",
      "4         3             Mwaqar (MWQAR)  36.127889  31.796571   171909   \n",
      "...      ..                        ...        ...        ...      ...   \n",
      "3810235  44  Tafila City (TAFILA_CITY)  35.640048  30.823988  3640892   \n",
      "3810236  44  Tafila City (TAFILA_CITY)  35.640048  30.823988  3640893   \n",
      "3810237  44  Tafila City (TAFILA_CITY)  35.640048  30.823988  3640894   \n",
      "3810238  44  Tafila City (TAFILA_CITY)  35.640048  30.823988  3640895   \n",
      "3810239  44  Tafila City (TAFILA_CITY)  35.640048  30.823988  3640896   \n",
      "\n",
      "                   date_time      power  weather_source_id  location_id  \n",
      "0        2022-01-16 16:15:00   0.002792                NaN            3  \n",
      "1        2022-01-16 16:30:00   0.002792                NaN            3  \n",
      "2        2022-01-16 16:45:00   0.002792                NaN            3  \n",
      "3        2022-01-16 17:00:00   0.002792                NaN            3  \n",
      "4        2022-01-16 17:15:00   0.002792                NaN            3  \n",
      "...                      ...        ...                ...          ...  \n",
      "3810235  2024-06-03 23:00:00  16.526047                NaN           44  \n",
      "3810236  2024-06-03 23:15:00  16.526047                NaN           44  \n",
      "3810237  2024-06-03 23:30:00  15.557803                NaN           44  \n",
      "3810238  2024-06-03 23:45:00  15.508034                NaN           44  \n",
      "3810239  2024-06-04 00:00:00  15.406828                NaN           44  \n",
      "\n",
      "[3810240 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Get all CSV file paths\n",
    "consumption_csv_path = r\"/Users/computer/Downloads/Load/Load/data//consumption_data/*.csv\"\n",
    "\n",
    "# Use glob to find all CSV files\n",
    "consumption_csv_files = glob.glob(consumption_csv_path)\n",
    "\n",
    "# Read and concatenate all CSV files\n",
    "df_list = [pd.read_csv(file) for file in consumption_csv_files]\n",
    "consumption_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(consumption_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61d4defb-4fff-405c-bdee-b59d132d417e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in name column: ['Mwaqar (MWQAR)' 'Rashadiya (RASHADIA)' \"Ma'an (MAAN)\" 'Disi (DISI)'\n",
      " 'El-Hasa (EL_HASA)' 'Zerqa (ZERQA)' 'Abdoon (ABDOON)' 'Qatrana (QATRANA)'\n",
      " 'Mwaqar Industrial (MWQAR IND)' 'City Center (CITY CENTER)'\n",
      " 'Salt (AL-SALT)' 'Azraq (AZRAQ)' 'Sabha (SABHA)' 'Hasheimya (HASHMYA)'\n",
      " 'Manara (MANARAH)' 'Irbid East (IRBID_E)' 'Queen Alia I. A. (Q.A.I.A.)'\n",
      " 'Amman South (AMMAN SOUTH)' 'Mafraq (MAFRAQ)' 'Irbid (IRBID_N)'\n",
      " 'Dulail (DHULEIL)' 'Bayader (BAYADER)' 'Waqas (WAQAS)' 'El-Risha (RESHA)'\n",
      " 'Marqa (MARQA)' 'Hasan Ind. Estate (HASAN IND)' 'Ishtafina (ISHTAFINA)'\n",
      " 'Tareq (TAREQ)' 'Madaba (MADABA SOUTH)'\n",
      " 'Aqaba Ind. Estate (AQABA INDUSTRIAL)' 'Abdali (ABDALI)'\n",
      " 'Shedia (SHEDIA)' 'Suweimeh (SWEIMEH)' 'Sahab (SAHAB)' 'Quwira (QUWEIRA)'\n",
      " 'Karak (KARAK)' 'Subeihi (SUBEIHI)' 'Ashrafia (ASHRFIA)'\n",
      " 'Ghorsafi (GHORSAFI)' 'University (UNIVRSTY)' '(AQABA THERMAL)'\n",
      " 'New Karak (NEW_KARAK)' 'Al-Hezam (AL HIZAM)' 'Al-Rama (RAMA)'\n",
      " 'Tafila City (TAFILA_CITY)']\n",
      "Number of unique values: 45\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check the unique values again\n",
    "print(\"Unique values in name column:\", consumption_df['name'].unique())\n",
    "print(\"Number of unique values:\", consumption_df['name'].nunique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d85b606-eea6-4343-9401-33684ffd355f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DateTime consumption Data:\n",
      "   id            name          x          y    id.1            date_time  \\\n",
      "0   3  Mwaqar (MWQAR)  36.127889  31.796571  171905  2022-01-16 16:15:00   \n",
      "1   3  Mwaqar (MWQAR)  36.127889  31.796571  171906  2022-01-16 16:30:00   \n",
      "2   3  Mwaqar (MWQAR)  36.127889  31.796571  171907  2022-01-16 16:45:00   \n",
      "3   3  Mwaqar (MWQAR)  36.127889  31.796571  171908  2022-01-16 17:00:00   \n",
      "4   3  Mwaqar (MWQAR)  36.127889  31.796571  171909  2022-01-16 17:15:00   \n",
      "\n",
      "      power  weather_source_id  location_id  \n",
      "0  0.002792                NaN            3  \n",
      "1  0.002792                NaN            3  \n",
      "2  0.002792                NaN            3  \n",
      "3  0.002792                NaN            3  \n",
      "4  0.002792                NaN            3  \n"
     ]
    }
   ],
   "source": [
    "# Display the resulting DataFrame\n",
    "print(\"Combined DateTime consumption Data:\")\n",
    "print(consumption_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d046ee26-afe4-4628-98c4-aa004ba63c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the DataFrame:\n",
      "Index(['id', 'name', 'x', 'y', 'id.1', 'date_time', 'power',\n",
      "       'weather_source_id', 'location_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Set pandas to display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Print the columns of the DataFrame\n",
    "print(\"Column names in the DataFrame:\")\n",
    "print(consumption_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51e8d30f-974c-4a20-aa5c-0ae927bfb170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                                    int64\n",
      "name                                 object\n",
      "source                               object\n",
      "lon                                 float64\n",
      "lat                                 float64\n",
      "alt                                 float64\n",
      "id.1                                  int64\n",
      "date_time                    datetime64[ns]\n",
      "horizontal_visibility_km            float64\n",
      "cloud_cover_oktas                   float64\n",
      "wind_speed_knot                     float64\n",
      "wind_direction_degree               float64\n",
      "irradiance                          float64\n",
      "relative_humidity_percent           float64\n",
      "air_temperature_c                   float64\n",
      "rainfall_mm                         float64\n",
      "weather_source_id                     int64\n",
      "pressure_mbar                       float64\n",
      "air_density_c                       float64\n",
      "module_temp_c                       float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(weather_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce22ca5c-0833-4a41-bacf-77c1396b2887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consumption DataFrame Columns: id                     int64\n",
      "name                  object\n",
      "x                    float64\n",
      "y                    float64\n",
      "id.1                   int64\n",
      "date_time             object\n",
      "power                float64\n",
      "weather_source_id    float64\n",
      "location_id            int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"consumption DataFrame Columns:\", consumption_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92feb806-7f34-4e28-9b0f-2e68bf91f767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the 'name' column in both dataframes for clarity\n",
    "consumption_df = consumption_df.rename(columns={'name': 'consumption_name'})\n",
    "weather_df = weather_df.rename(columns={'name': 'weather_name'})\n",
    "\n",
    "# Now you can use these new column names in the code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec271204-6b35-4981-8616-66b1a84e9639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(weather_df['date_time'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84675f86-c4b4-4f8b-916f-436c637d006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'date_time' is set as the index while keeping it as a column\n",
    "weather_df = weather_df.set_index('date_time')  # Set 'date_time' as index for time-based operations\n",
    "weather_df['date_time'] = weather_df.index  # Keep 'date_time' as a column for easy access\n",
    "\n",
    "consumption_df = consumption_df.set_index('date_time')  # Set 'date_time' as index for time-based operations\n",
    "consumption_df['date_time'] = consumption_df.index  # Keep 'date_time' as a column for easy access\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6493d4f4-9646-42b8-baf5-4214f3fb45eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(weather_df.columns[weather_df.columns.duplicated()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b12d414-f23c-41c5-9309-2e94826b40c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "# Show unique values in the 'name' column from consumption df and  from weather df\n",
    "print(consumption_df['consumption_name'].nunique())\n",
    "print(weather_df['weather_name'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "330b1a4f-186c-4092-b041-f24df3796ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique consumption stations: 45\n",
      "['Mwaqar (MWQAR)' 'Rashadiya (RASHADIA)' \"Ma'an (MAAN)\" 'Disi (DISI)'\n",
      " 'El-Hasa (EL_HASA)' 'Zerqa (ZERQA)' 'Abdoon (ABDOON)' 'Qatrana (QATRANA)'\n",
      " 'Mwaqar Industrial (MWQAR IND)' 'City Center (CITY CENTER)'\n",
      " 'Salt (AL-SALT)' 'Azraq (AZRAQ)' 'Sabha (SABHA)' 'Hasheimya (HASHMYA)'\n",
      " 'Manara (MANARAH)' 'Irbid East (IRBID_E)' 'Queen Alia I. A. (Q.A.I.A.)'\n",
      " 'Amman South (AMMAN SOUTH)' 'Mafraq (MAFRAQ)' 'Irbid (IRBID_N)'\n",
      " 'Dulail (DHULEIL)' 'Bayader (BAYADER)' 'Waqas (WAQAS)' 'El-Risha (RESHA)'\n",
      " 'Marqa (MARQA)' 'Hasan Ind. Estate (HASAN IND)' 'Ishtafina (ISHTAFINA)'\n",
      " 'Tareq (TAREQ)' 'Madaba (MADABA SOUTH)'\n",
      " 'Aqaba Ind. Estate (AQABA INDUSTRIAL)' 'Abdali (ABDALI)'\n",
      " 'Shedia (SHEDIA)' 'Suweimeh (SWEIMEH)' 'Sahab (SAHAB)' 'Quwira (QUWEIRA)'\n",
      " 'Karak (KARAK)' 'Subeihi (SUBEIHI)' 'Ashrafia (ASHRFIA)'\n",
      " 'Ghorsafi (GHORSAFI)' 'University (UNIVRSTY)' '(AQABA THERMAL)'\n",
      " 'New Karak (NEW_KARAK)' 'Al-Hezam (AL HIZAM)' 'Al-Rama (RAMA)'\n",
      " 'Tafila City (TAFILA_CITY)']\n"
     ]
    }
   ],
   "source": [
    "# Count unique consumption stations names\n",
    "unique_stations_count = consumption_df['consumption_name'].nunique()\n",
    "print(f\"Number of unique consumption stations: {unique_stations_count}\")\n",
    "\n",
    "# Display unique consumption station names\n",
    "print(consumption_df['consumption_name'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff5155cb-6409-4252-b3cd-dd304698c92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique consumption stations: 31\n",
      "['Queen Alia International Airport' 'Shoubak' 'King Hussien'\n",
      " 'Wadi El-rayyan' 'Azraq' 'Aqaba Port' 'Qatraneh' 'Er Rabbah' 'Ghour Safi'\n",
      " 'Al Jafer' 'Salt' 'Jarash' 'Mafraq' 'Safawi' 'Rewaished' 'Ghabawi'\n",
      " 'Zarqa' 'Irbid' 'Ramtha' 'Dhulail' 'Quiesmeh' 'Amman Civil Airport'\n",
      " 'Downtown' 'Hussien Garden' 'Jubiha' 'Baqura' 'Dier alla' \"Ma'an\"\n",
      " 'Wadi Mousa - Petra' 'Tafileh' 'Ras Muneeef']\n"
     ]
    }
   ],
   "source": [
    "# Count unique weather stations names\n",
    "unique_stations_count = weather_df['weather_name'].nunique()\n",
    "print(f\"Number of unique consumption stations: {unique_stations_count}\")\n",
    "\n",
    "# Display unique consumption station names\n",
    "print(weather_df['weather_name'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c786b3db-d4e5-4687-bee0-84acda068f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in consumption_df:\n",
      "   Missing Count  Missing Percentage\n",
      "x          84672            2.222222\n",
      "y          84672            2.222222\n",
      "\n",
      "Consumption stations with missing coordinates:\n",
      "['(AQABA THERMAL)']\n",
      "\n",
      "Missing values in weather_df:\n",
      "     Missing Count  Missing Percentage\n",
      "lon              0                 0.0\n",
      "lat              0                 0.0\n",
      "\n",
      "No missing weather station coordinates found.\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate missing values and percentage\n",
    "def missing_info(df, cols):\n",
    "    missing_count = df[cols].isna().sum()  # Count missing values\n",
    "    missing_percentage = (missing_count / len(df)) * 100  # Compute percentage\n",
    "    return pd.DataFrame({'Missing Count': missing_count, 'Missing Percentage': missing_percentage})\n",
    "\n",
    "# Check for NaNs in consumption_df\n",
    "print(\"Missing values in consumption_df:\")\n",
    "print(missing_info(consumption_df, ['x', 'y']))\n",
    "\n",
    "# Get the names of consumption stations with missing coordinates\n",
    "missing_stations = consumption_df.loc[consumption_df[['x', 'y']].isna().any(axis=1), 'consumption_name']\n",
    "\n",
    "# Display the names of stations with missing data\n",
    "if not missing_stations.empty:\n",
    "    print(\"\\nConsumption stations with missing coordinates:\")\n",
    "    print(missing_stations.unique())\n",
    "else:\n",
    "    print(\"\\nNo missing consumption station coordinates found.\")\n",
    "\n",
    "# Check for NaNs in weather_df\n",
    "print(\"\\nMissing values in weather_df:\")\n",
    "print(missing_info(weather_df, ['lon', 'lat']))\n",
    "\n",
    "# Get the names of weather stations with missing coordinates\n",
    "missing_weather_stations = weather_df.loc[weather_df[['lon', 'lat']].isna().any(axis=1), 'weather_name']\n",
    "\n",
    "# Display the names of weather stations with missing data\n",
    "if not missing_weather_stations.empty:\n",
    "    print(\"\\nWeather stations with missing coordinates:\")\n",
    "    print(missing_weather_stations.unique())\n",
    "else:\n",
    "    print(\"\\nNo missing weather station coordinates found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c923420-3956-468c-8da3-02da992b14f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values after filling for AQABA THERMAL station:\n",
      "id                   0\n",
      "consumption_name     0\n",
      "x                    0\n",
      "y                    0\n",
      "id.1                 0\n",
      "power                0\n",
      "weather_source_id    0\n",
      "location_id          0\n",
      "date_time            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# AQABA THERMAL station coordinates (latitude, longitude)\n",
    "aqaba_coords = (29.3772, 34.9779)\n",
    "\n",
    "# Specify the station name you're targeting\n",
    "station_name = 'AQABA THERMAL'  # Replace with the appropriate station name if different\n",
    "\n",
    "# Fill missing 'x' and 'y' coordinates with AQABA THERMAL coordinates\n",
    "consumption_df.loc[consumption_df['consumption_name'] == station_name, 'x'] = consumption_df.loc[consumption_df['consumption_name'] == station_name, 'x'].fillna(aqaba_coords[0])\n",
    "consumption_df.loc[consumption_df['consumption_name'] == station_name, 'y'] = consumption_df.loc[consumption_df['consumption_name'] == station_name, 'y'].fillna(aqaba_coords[1])\n",
    "\n",
    "# Fill missing numeric columns with the median of the AQABA THERMAL station\n",
    "numeric_cols = consumption_df.select_dtypes(include=['number']).columns\n",
    "consumption_df.loc[consumption_df['consumption_name'] == station_name, numeric_cols] = consumption_df.loc[consumption_df['consumption_name'] == station_name, numeric_cols].fillna(consumption_df.loc[consumption_df['consumption_name'] == station_name, numeric_cols].median())\n",
    "\n",
    "# For categorical columns, fill NaN with the mode value or use 'Unknown' as a fallback if mode is empty\n",
    "categorical_cols = consumption_df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    mode_value = consumption_df.loc[consumption_df['consumption_name'] == station_name, col].mode()\n",
    "    if not mode_value.empty:\n",
    "        mode_value = mode_value[0]\n",
    "    else:\n",
    "        mode_value = 'Unknown'  # Fallback value if mode is empty\n",
    "    consumption_df.loc[consumption_df['consumption_name'] == station_name, col] = consumption_df.loc[consumption_df['consumption_name'] == station_name, col].fillna(mode_value)\n",
    "\n",
    "# Check the result after filling missing values\n",
    "print(\"\\nMissing values after filling for AQABA THERMAL station:\")\n",
    "print(consumption_df.loc[consumption_df['consumption_name'] == station_name].isna().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd3b2397-2722-4aa8-a4c0-e97b8a4649b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, weather_row \u001b[38;5;129;01min\u001b[39;00m weather_df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     22\u001b[0m     weather_coords \u001b[38;5;241m=\u001b[39m (weather_row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m'\u001b[39m], weather_row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# (Lat, Lon)\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     distance \u001b[38;5;241m=\u001b[39m \u001b[43mgeodesic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconsumption_coords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweather_coords\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mkm  \u001b[38;5;66;03m# Distance in kilometers\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Update if we find a closer station\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m distance \u001b[38;5;241m<\u001b[39m min_distance:\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/geopy/distance.py:540\u001b[0m, in \u001b[0;36mgeodesic.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_ellipsoid(kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mellipsoid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWGS-84\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    539\u001b[0m major, minor, f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mELLIPSOID\n\u001b[0;32m--> 540\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/geopy/distance.py:276\u001b[0m, in \u001b[0;36mDistance.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m util\u001b[38;5;241m.\u001b[39mpairwise(args):\n\u001b[0;32m--> 276\u001b[0m         kilometers \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeasure\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m kilometers \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m units\u001b[38;5;241m.\u001b[39mkilometers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kilometers \u001b[38;5;241m=\u001b[39m kilometers\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/geopy/distance.py:566\u001b[0m, in \u001b[0;36mgeodesic.measure\u001b[0;34m(self, a, b)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeod, Geodesic) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeod\u001b[38;5;241m.\u001b[39ma \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mELLIPSOID[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    563\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeod\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mELLIPSOID[\u001b[38;5;241m2\u001b[39m]):\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeod \u001b[38;5;241m=\u001b[39m Geodesic(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mELLIPSOID[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mELLIPSOID[\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m--> 566\u001b[0m s12 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlat1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlon1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlat2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlon2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mGeodesic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDISTANCE\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms12\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m s12\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/geographiclib/geodesic.py:1030\u001b[0m, in \u001b[0;36mGeodesic.Inverse\u001b[0;34m(self, lat1, lon1, lat2, lon2, outmask)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mInverse\u001b[39m(\u001b[38;5;28mself\u001b[39m, lat1, lon1, lat2, lon2,\n\u001b[1;32m   1013\u001b[0m             outmask \u001b[38;5;241m=\u001b[39m GeodesicCapability\u001b[38;5;241m.\u001b[39mSTANDARD):\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Solve the inverse geodesic problem\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \n\u001b[1;32m   1016\u001b[0m \u001b[38;5;124;03m  :param lat1: latitude of the first point in degrees\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1027\u001b[0m \n\u001b[1;32m   1028\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1030\u001b[0m   a12, s12, salp1,calp1, salp2,calp2, m12, M12, M21, S12 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_GenInverse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlat1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlon1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlat2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlon2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m   outmask \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m Geodesic\u001b[38;5;241m.\u001b[39mOUT_MASK\n\u001b[1;32m   1033\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m outmask \u001b[38;5;241m&\u001b[39m Geodesic\u001b[38;5;241m.\u001b[39mLONG_UNROLL:\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/geographiclib/geodesic.py:876\u001b[0m, in \u001b[0;36mGeodesic._GenInverse\u001b[0;34m(self, lat1, lon1, lat2, lon2, outmask)\u001b[0m\n\u001b[1;32m    870\u001b[0m salp1b \u001b[38;5;241m=\u001b[39m Geodesic\u001b[38;5;241m.\u001b[39mtiny_; calp1b \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m numit \u001b[38;5;241m<\u001b[39m Geodesic\u001b[38;5;241m.\u001b[39mmaxit2_:\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# the WGS84 test set: mean = 1.47, sd = 1.25, max = 16\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;66;03m# WGS84 and random input: mean = 2.85, sd = 0.60\u001b[39;00m\n\u001b[1;32m    875\u001b[0m   (v, salp2, calp2, sig12, ssig1, csig1, ssig2, csig2,\n\u001b[0;32m--> 876\u001b[0m    eps, domg12, dv) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Lambda12\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m     \u001b[49m\u001b[43msbet1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbet1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdn1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msbet2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbet2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdn2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m     \u001b[49m\u001b[43msalp1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalp1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslam12\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclam12\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mGeodesic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxit1_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m     \u001b[49m\u001b[43mC1a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC2a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC3a\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m   \u001b[38;5;66;03m# Reversed test to allow escape with NaNs\u001b[39;00m\n\u001b[1;32m    881\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m tripb \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mabs\u001b[39m(v) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tripn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m Geodesic\u001b[38;5;241m.\u001b[39mtol0_):\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/geographiclib/geodesic.py:690\u001b[0m, in \u001b[0;36mGeodesic._Lambda12\u001b[0;34m(self, sbet1, cbet1, dn1, sbet2, cbet2, dn2, salp1, calp1, slam120, clam120, diffp, C1a, C2a, C3a)\u001b[0m\n\u001b[1;32m    688\u001b[0m     dlam12 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_f1 \u001b[38;5;241m*\u001b[39m dn1 \u001b[38;5;241m/\u001b[39m sbet1\n\u001b[1;32m    689\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 690\u001b[0m     dummy, dlam12, dummy, dummy, dummy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Lengths\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m      \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msig12\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mssig1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsig1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdn1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mssig2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsig2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdn2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbet1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbet2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mGeodesic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREDUCEDLENGTH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC1a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC2a\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m     dlam12 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_f1 \u001b[38;5;241m/\u001b[39m (calp2 \u001b[38;5;241m*\u001b[39m cbet2)\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/geographiclib/geodesic.py:447\u001b[0m, in \u001b[0;36mGeodesic._Lengths\u001b[0;34m(self, eps, sig12, ssig1, csig1, dn1, ssig2, csig2, dn2, cbet1, cbet2, outmask, C1a, C2a)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outmask \u001b[38;5;241m&\u001b[39m (Geodesic\u001b[38;5;241m.\u001b[39mDISTANCE \u001b[38;5;241m|\u001b[39m Geodesic\u001b[38;5;241m.\u001b[39mREDUCEDLENGTH \u001b[38;5;241m|\u001b[39m\n\u001b[1;32m    445\u001b[0m               Geodesic\u001b[38;5;241m.\u001b[39mGEODESICSCALE):\n\u001b[1;32m    446\u001b[0m   A1 \u001b[38;5;241m=\u001b[39m Geodesic\u001b[38;5;241m.\u001b[39m_A1m1f(eps)\n\u001b[0;32m--> 447\u001b[0m   \u001b[43mGeodesic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C1f\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC1a\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m outmask \u001b[38;5;241m&\u001b[39m (Geodesic\u001b[38;5;241m.\u001b[39mREDUCEDLENGTH \u001b[38;5;241m|\u001b[39m Geodesic\u001b[38;5;241m.\u001b[39mGEODESICSCALE):\n\u001b[1;32m    449\u001b[0m     A2 \u001b[38;5;241m=\u001b[39m Geodesic\u001b[38;5;241m.\u001b[39m_A2m1f(eps)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/geographiclib/geodesic.py:205\u001b[0m, in \u001b[0;36mGeodesic._C1f\u001b[0;34m(eps, c)\u001b[0m\n\u001b[1;32m    202\u001b[0m   t \u001b[38;5;241m=\u001b[39m Math\u001b[38;5;241m.\u001b[39mpolyval(m, coeff, \u001b[38;5;241m0\u001b[39m, Math\u001b[38;5;241m.\u001b[39msq(eps)) \u001b[38;5;241m/\u001b[39m coeff[m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    203\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (t \u001b[38;5;241m+\u001b[39m eps) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m eps)\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_C1f\u001b[39m(eps, c):\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Private: return C1.\"\"\"\u001b[39;00m\n\u001b[1;32m    208\u001b[0m   coeff \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m2048\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m2048\u001b[39m,\n\u001b[1;32m    215\u001b[0m   ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Remove rows with NaN values in the relevant columns\n",
    "consumption_df = consumption_df.dropna(subset=['x', 'y'])\n",
    "weather_df = weather_df.dropna(subset=['lon', 'lat'])\n",
    "\n",
    "# Initialize an empty list to store the nearest weather station names and their distances\n",
    "nearest_station_names = []\n",
    "nearest_station_distances = []\n",
    "\n",
    "# Loop through each consumption station\n",
    "for _, consumption_row in consumption_df.iterrows():\n",
    "    consumption_coords = (consumption_row['y'], consumption_row['x'])  # (Lat, Lon)\n",
    "\n",
    "    # Initialize variables to find the closest station\n",
    "    closest_station = None\n",
    "    min_distance = float('inf')\n",
    "\n",
    "    # Loop through each weather station to compute the distance\n",
    "    for _, weather_row in weather_df.iterrows():\n",
    "        weather_coords = (weather_row['lat'], weather_row['lon'])  # (Lat, Lon)\n",
    "        distance = geodesic(consumption_coords, weather_coords).km  # Distance in kilometers\n",
    "\n",
    "        # Update if we find a closer station\n",
    "        if distance < min_distance:\n",
    "            closest_station = weather_row['weather_name']\n",
    "            min_distance = distance\n",
    "\n",
    "    # Append the results for the nearest station\n",
    "    nearest_station_names.append(closest_station)\n",
    "    nearest_station_distances.append(min_distance)\n",
    "\n",
    "# Add the nearest weather station and the distance to the consumption dataframe\n",
    "consumption_df['nearest_weather_station'] = nearest_station_names\n",
    "consumption_df['distance_to_nearest_station'] = nearest_station_distances\n",
    "\n",
    "# Merge the consumption data with the weather data based on 'nearest_weather_station'\n",
    "merged_df = consumption_df.merge(weather_df, left_on='nearest_weather_station', right_on='weather_name', how='left')\n",
    "\n",
    "# Display the merged data\n",
    "print(merged_df[['consumption_name', 'nearest_weather_station', 'weather_name', 'distance_to_nearest_station', 'other_weather_data_column']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bfd875-e57a-4d45-8206-87c8a684ef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the columns of consumption_df to see the actual column names\n",
    "print(\"Columns of consumption_df:\")\n",
    "print(consumption_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600e56fa-fd0d-42a1-98cc-4129e5414c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the columns of weather_df to see the actual column names\n",
    "print(\"Columns of weather_df:\")\n",
    "print(weather_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2380454-6c51-45fd-ab38-27b29d62a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(consumption_df.columns)\n",
    "print(weather_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac9ef0d-2f10-42a4-bacc-cba916236e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure 'date_time' is in datetime format for both DataFrames\n",
    "consumption_df['date_time'] = pd.to_datetime(consumption_df['date_time'])\n",
    "weather_df['date_time'] = pd.to_datetime(weather_df['date_time'])\n",
    "\n",
    "# Ensure the datasets have the same date range by finding the overlap\n",
    "start_date = max(consumption_df['date_time'].min(), weather_df['date_time'].min())\n",
    "end_date = min(consumption_df['date_time'].max(), weather_df['date_time'].max())\n",
    "\n",
    "# Output the start and end dates\n",
    "print(f\"Start date: {start_date}\")\n",
    "print(f\"End date: {end_date}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e30a73-407f-4253-9902-80e72db55c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the column names in the consumption dataframe\n",
    "print(\"Consumption DataFrame Columns:\")\n",
    "print(consumption_df.columns)\n",
    "\n",
    "# Check the first few values of the relevant column in the consumption dataframe\n",
    "print(\"\\nFirst few rows of 'consumption_name' column:\")\n",
    "print(consumption_df['consumption_name'].head())\n",
    "\n",
    "# Check the column names in the weather dataframe\n",
    "print(\"\\nWeather DataFrame Columns:\")\n",
    "print(weather_df.columns)\n",
    "\n",
    "# Check the first few values of the relevant column in the weather dataframe\n",
    "print(\"\\nFirst few rows of 'weather_name' column:\")\n",
    "print(weather_df['weather_name'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494e168c-97e7-4265-965e-8a62733f4803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# Define the central coordinates for the map (center of Jordan)\n",
    "center_lat = 31.5  # Approximate latitude of Jordan's center\n",
    "center_lon = 36.5  # Approximate longitude of Jordan's center\n",
    "\n",
    "# Create a map centered around Jordan\n",
    "mymap = folium.Map(location=[center_lat, center_lon], zoom_start=7)\n",
    "\n",
    "# Add unique consumption stations to the map (with blue color)\n",
    "unique_consumption_df = consumption_df.drop_duplicates(subset=['x', 'y'])\n",
    "\n",
    "for _, row in unique_consumption_df.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['y'], row['x']],  # [lat, lon]\n",
    "        radius=5,\n",
    "        color='blue',  # Color for consumption stations\n",
    "        fill=True,\n",
    "        fill_color='blue',\n",
    "        fill_opacity=0.6,\n",
    "        popup=row['consumption_name']  # Show the consumption station name on click\n",
    "    ).add_to(mymap)\n",
    "\n",
    "# Add unique weather stations to the map (with red color)\n",
    "unique_weather_df = weather_df.drop_duplicates(subset=['lon', 'lat'])\n",
    "\n",
    "for _, row in unique_weather_df.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['lat'], row['lon']],  # [lat, lon]\n",
    "        popup=row['weather_name'],  # Show the weather station name on click\n",
    "        icon=folium.Icon(color='red')  # Red color for weather stations\n",
    "    ).add_to(mymap)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "mymap.save(\"jordan_stations_unique_map.html\")\n",
    "\n",
    "# Optionally, display the map inline (if you're using Jupyter Notebook)\n",
    "mymap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd2ffbf-02bf-4275-b4a3-885523b2ddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import pandas as pd\n",
    "\n",
    "# Function to classify climate zones based on multiple factors\n",
    "def classify_climate_zone(row):\n",
    "    # Arid Climates\n",
    "    if row['rainfall_mm'] < 250:\n",
    "        if row['air_temperature_c'] > 20:\n",
    "            return 'Arabian Desert Climate'  # BWh (Hot Desert)\n",
    "        else:\n",
    "            return 'Irano-Turanian Climate'  # BSk (Cold Desert)\n",
    "    \n",
    "    # Tropical Climates\n",
    "    elif row['rainfall_mm'] > 1000:\n",
    "        if row['air_temperature_c'] > 25:\n",
    "            return 'Tropical Climate'  # Af or Am\n",
    "        else:\n",
    "            return 'Tropical Climate'  # Aw (Tropical Savanna)\n",
    "    \n",
    "    # Temperate Climates\n",
    "    elif 10 <= row['air_temperature_c'] <= 25 and row['rainfall_mm'] > 250:\n",
    "        return 'Mediterranean Climate'  # Csa, Csb\n",
    "    \n",
    "    # Mountain Climate\n",
    "    elif row['alt'] > 1000 and row['air_temperature_c'] < 10:\n",
    "        return 'Mountain Climate'  # High altitude with cool temperatures\n",
    "    \n",
    "    # Sudanese Climate\n",
    "    elif 400 <= row['rainfall_mm'] <= 800 and row['air_temperature_c'] > 30:\n",
    "        return 'Sudanese Climate'  # Tropical wet climate\n",
    "    \n",
    "    # Default classification (if none of the above conditions are met)\n",
    "    return 'Unclassified'\n",
    "\n",
    "# Assuming 'weather_df' has columns 'rainfall_mm', 'air_temperature_c', and 'alt' for each weather station\n",
    "weather_df['weather_zone'] = weather_df.apply(classify_climate_zone, axis=1)\n",
    "\n",
    "# Map creation\n",
    "center_lat = 31.5  # Approximate latitude of Jordan's center\n",
    "center_lon = 36.5  # Approximate longitude of Jordan's center\n",
    "\n",
    "mymap = folium.Map(location=[center_lat, center_lon], zoom_start=7)\n",
    "\n",
    "# Create a color map for the weather zones\n",
    "zone_colors = {\n",
    "    'Arabian Desert Climate': 'red',\n",
    "    'Irano-Turanian Climate': 'orange',\n",
    "    'Tropical Climate': 'yellow',\n",
    "    'Mediterranean Climate': 'green',\n",
    "    'Mountain Climate': 'blue',\n",
    "    'Sudanese Climate': 'purple',\n",
    "    'Unclassified': 'gray'\n",
    "}\n",
    "\n",
    "# Add weather zones as polygons or circles on the map\n",
    "for _, row in weather_df.iterrows():\n",
    "    zone_color = zone_colors.get(row['weather_zone'], 'gray')\n",
    "    \n",
    "    folium.CircleMarker(\n",
    "        location=[row['lat'], row['lon']],  # Coordinates of weather station\n",
    "        radius=8,\n",
    "        color=zone_color,  # Color based on climate zone\n",
    "        fill=True,\n",
    "        fill_color=zone_color,\n",
    "        fill_opacity=0.6,\n",
    "        popup=f\"{row['weather_name']} - {row['weather_zone']}\"  # Show the zone name on click\n",
    "    ).add_to(mymap)\n",
    "\n",
    "# Add unique consumption stations to the map (with blue color)\n",
    "unique_consumption_df = consumption_df.drop_duplicates(subset=['x', 'y'])\n",
    "\n",
    "for _, row in unique_consumption_df.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['y'], row['x']],  # [lat, lon]\n",
    "        radius=5,\n",
    "        color='blue',  # Color for consumption stations\n",
    "        fill=True,\n",
    "        fill_color='blue',\n",
    "        fill_opacity=0.6,\n",
    "        popup=row['consumption_name']  # Show the consumption station name on click\n",
    "    ).add_to(mymap)\n",
    "\n",
    "# Add unique weather stations to the map (with red color)\n",
    "unique_weather_df = weather_df.drop_duplicates(subset=['lon', 'lat'])\n",
    "\n",
    "for _, row in unique_weather_df.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['lat'], row['lon']],  # [lat, lon]\n",
    "        popup=row['weather_name'],  # Show the weather station name on click\n",
    "        icon=folium.Icon(color='red')  # Red color for weather stations\n",
    "    ).add_to(mymap)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "mymap.save(\"jordan_stations_weather_zones_map.html\")\n",
    "\n",
    "# Optionally, display the map inline (if you're using Jupyter Notebook)\n",
    "mymap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2166aa-9d19-40bc-8615-8fe1210e3884",
   "metadata": {},
   "source": [
    "# Find weather zones from data\n",
    "## Preprocess and Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6900688-33d5-4bdc-b24b-975adc89b863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Clean the data by dropping rows with NaN values in relevant columns\n",
    "weather_df = weather_df.dropna(subset=['wind_speed_knot', 'wind_direction_degree', 'irradiance', \n",
    "                                       'relative_humidity_percent', 'air_temperature_c', 'rainfall_mm'])\n",
    "\n",
    "# Optionally, you could check for duplicate rows and remove them\n",
    "weather_df = weather_df.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cb9bfd-fcf7-436a-9651-875c07395b31",
   "metadata": {},
   "source": [
    "## Normalize or Standardize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595b9a08-c8ee-4b24-b7ec-9ef7ba3378e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardizing the weather-related variables\n",
    "scaler = StandardScaler()\n",
    "weather_df[['wind_speed_knot', 'wind_direction_degree', 'irradiance', \n",
    "            'relative_humidity_percent', 'air_temperature_c', 'rainfall_mm']] = \\\n",
    "    scaler.fit_transform(weather_df[['wind_speed_knot', 'wind_direction_degree', 'irradiance', \n",
    "                                     'relative_humidity_percent', 'air_temperature_c', 'rainfall_mm']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99840a14-d30e-4913-bc62-e64fab75974a",
   "metadata": {},
   "source": [
    "## Apply Clustering to Define Weather Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b1b819-cfe5-48a5-b21d-274ae40b2a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the columns of the weather_df\n",
    "print(\"Columns of weather_df:\")\n",
    "print(weather_df.columns)\n",
    "\n",
    "# List the columns of the consumption_df\n",
    "print(\"\\nColumns of consumption_df:\")\n",
    "print(consumption_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7009da-a94e-4380-9ce7-57fa52426429",
   "metadata": {},
   "source": [
    "##  Visualize Weather Zones on a Map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae078475-72c1-4209-b67b-64c32a849009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to classify climate zones based on multiple factors\n",
    "def classify_climate_zone(row):\n",
    "    # Arid Climates\n",
    "    if row['rainfall_mm'] < 250:\n",
    "        if row['air_temperature_c'] > 20:\n",
    "            return 'Arabian Desert Climate'  # BWh (Hot Desert)\n",
    "        else:\n",
    "            return 'Irano-Turanian Climate'  # BSk (Cold Desert)\n",
    "    \n",
    "    # Tropical Climates\n",
    "    elif row['rainfall_mm'] > 1000:\n",
    "        if row['air_temperature_c'] > 25:\n",
    "            return 'Tropical Climate'  # Af or Am\n",
    "        else:\n",
    "            return 'Tropical Climate'  # Aw (Tropical Savanna)\n",
    "    \n",
    "    # Temperate Climates\n",
    "    elif 10 <= row['air_temperature_c'] <= 25 and row['rainfall_mm'] > 250:\n",
    "        return 'Mediterranean Climate'  # Csa, Csb\n",
    "    \n",
    "    # Mountain Climate\n",
    "    elif row['alt'] > 1000 and row['air_temperature_c'] < 10:\n",
    "        return 'Mountain Climate'  # High altitude with cool temperatures\n",
    "    \n",
    "    # Sudanese Climate\n",
    "    elif 400 <= row['rainfall_mm'] <= 800 and row['air_temperature_c'] > 30:\n",
    "        return 'Sudanese Climate'  # Tropical wet climate\n",
    "    \n",
    "    # Default classification (if none of the above conditions are met)\n",
    "    return 'Unclassified'\n",
    "\n",
    "# Apply the classification function to each weather station\n",
    "weather_df['weather_zone'] = weather_df.apply(classify_climate_zone, axis=1)\n",
    "\n",
    "# Display the weather stations with their assigned climate zones\n",
    "print(weather_df[['weather_name', 'weather_zone']])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874ced86-61ef-4aac-8695-56d97de20497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216f55fc-c11c-4c9f-b66a-ee86d539e1ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a0b575-783b-49fd-901d-189a218b1b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "# Create a map centered on Jordan (adjust based on your coordinates)\n",
    "map_center = [31.5, 35.5]  # Adjust based on Jordan's coordinates\n",
    "m = folium.Map(location=map_center, zoom_start=7)\n",
    "\n",
    "# Create a marker cluster for better visualization\n",
    "marker_cluster = MarkerCluster().add_to(m)\n",
    "\n",
    "# Loop through the weather_df to add markers for each weather station with their zone\n",
    "for _, row in weather_df.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['lat'], row['lon']],\n",
    "        popup=f\"Weather Station: {row['weather_name']}<br>Zone: {row['weather_zone']}<br>Nearest Consumption: {row['nearest_consumption_station']}\",\n",
    "        icon=folium.Icon(color='blue' if row['weather_zone'] == 'Mediterranean Climate' else 'red')\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "# Loop through the consumption_df to add markers for each consumption station\n",
    "for _, row in consumption_df.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['x'], row['y']],\n",
    "        popup=f\"Consumption Station: {row['consumption_name']}\",\n",
    "        icon=folium.Icon(color='green')\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "m.save('weather_consumption_map_with_zones.html')\n",
    "\n",
    "# If running in a Jupyter notebook, display the map inline\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36402719-9fb0-4b1f-b292-426c8af2e6f6",
   "metadata": {},
   "source": [
    "## clustering weather zones finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e372e8-7bf7-428e-80df-5217857522ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Select relevant columns for clustering\n",
    "weather_data = weather_df[['wind_speed_knot', 'wind_direction_degree', 'irradiance', \n",
    "                           'relative_humidity_percent', 'air_temperature_c', 'rainfall_mm']]\n",
    "\n",
    "# Apply K-Means clustering to classify stations into different weather zones\n",
    "# Setting n_clusters to 4 for the four main climate zones in Jordan\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)  # 4 weather zones for Jordan\n",
    "weather_df['weather_zone'] = kmeans.fit_predict(weather_data)\n",
    "\n",
    "# Map the weather zones to the actual climate zones of Jordan\n",
    "zone_labels = {0: 'Desert Zone', 1: 'Semi-Arid Zone', 2: 'Mediterranean Zone', 3: 'Highland Zone'}\n",
    "weather_df['zone_label'] = weather_df['weather_zone'].map(zone_labels)\n",
    "\n",
    "# Display the stations with their assigned weather zones\n",
    "print(weather_df[['weather_source_id', 'zone_label']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b5b4ab-22c4-4434-b504-530701305f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# Define the central coordinates for the map (center of Jordan)\n",
    "center_lat = 31.5  # Approximate latitude of Jordan's center\n",
    "center_lon = 36.5  # Approximate longitude of Jordan's center\n",
    "\n",
    "# Create a map centered around Jordan\n",
    "mymap = folium.Map(location=[center_lat, center_lon], zoom_start=7)\n",
    "\n",
    "# Define color palette for the four weather zones in Jordan\n",
    "zone_colors = {\n",
    "    'Desert Zone': 'orange',\n",
    "    'Semi-Arid Zone': 'yellow',\n",
    "    'Mediterranean Zone': 'green',\n",
    "    'Highland Zone': 'blue'\n",
    "}\n",
    "\n",
    "# Add unique consumption stations to the map (without zone label)\n",
    "for _, row in consumption_df.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['y'], row['x']],  # Latitude and Longitude of consumption stations\n",
    "        radius=5,\n",
    "        color='black',  # Black color for consumption stations\n",
    "        fill=True,\n",
    "        fill_color='black',\n",
    "        fill_opacity=0.6,\n",
    "        popup=f\"Consumption Station: {row['consumption_name']}\"  # Show consumption station info\n",
    "    ).add_to(mymap)\n",
    "\n",
    "# Add unique weather stations to the map (colored by their zone label)\n",
    "for _, row in weather_df.iterrows():\n",
    "    zone = row['zone_label']  # Weather zone for the station\n",
    "    color = zone_colors.get(zone, 'gray')  # Assign color based on the zone label\n",
    "    \n",
    "    folium.CircleMarker(\n",
    "        location=[row['lat'], row['lon']],  # Latitude and Longitude of weather stations\n",
    "        radius=5,\n",
    "        color=color,  # Color based on the zone\n",
    "        fill=True,\n",
    "        fill_color=color,\n",
    "        fill_opacity=0.6,\n",
    "        popup=f\"Weather Station: {row['weather_name']} | Zone: {zone}\"  # Show weather station info with zone label\n",
    "    ).add_to(mymap)\n",
    "\n",
    "# Add a legend to the map\n",
    "legend_html = \"\"\"\n",
    "     <div style=\"position: fixed; \n",
    "                 bottom: 50px; left: 50px; width: 150px; height: 150px; \n",
    "                 border:2px solid grey; background-color:white; z-index:9999; \n",
    "                 font-size:14px; padding: 10px;\">\n",
    "         <b>Weather Zones</b><br>\n",
    "         <i style=\"background:orange; width:20px; height:20px; float:left; margin-right:5px;\"></i> Desert Zone<br>\n",
    "         <i style=\"background:yellow; width:20px; height:20px; float:left; margin-right:5px;\"></i> Semi-Arid Zone<br>\n",
    "         <i style=\"background:green; width:20px; height:20px; float:left; margin-right:5px;\"></i> Mediterranean Zone<br>\n",
    "         <i style=\"background:blue; width:20px; height:20px; float:left; margin-right:5px;\"></i> Highland Zone<br>\n",
    "     </div>\n",
    "     \"\"\"\n",
    "mymap.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "# Save the map to an HTML file\n",
    "mymap.save(\"weather_and_consumption_zones_with_weather_station_labels.html\")\n",
    "\n",
    "# Optionally, display the map inline (if you're using Jupyter Notebook)\n",
    "mymap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03961864-5591-40e7-b2df-c2cc13fc1c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resample weather dataframe to be in 15 miinuites to merge consumption and weather dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a82f9d-f1ac-49d4-9147-043803c1e615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'date_time' is set as the index before resampling\n",
    "weather_df['date_time'] = pd.to_datetime(weather_df['date_time'])  # Ensure it's in datetime format\n",
    "weather_df = weather_df.set_index('date_time')\n",
    "\n",
    "# Save the 'weather_name' column before resampling\n",
    "weather_names = weather_df['weather_name']\n",
    "\n",
    "# Resample the data (only numeric columns)\n",
    "numeric_cols = weather_df.select_dtypes(include=['number']).columns\n",
    "weather_df = weather_df[numeric_cols].resample('15min').mean()\n",
    "\n",
    "# Add back the 'weather_name' column after resampling\n",
    "# Forward fill the 'weather_name' to match the resampled data\n",
    "weather_df['weather_name'] = weather_names.resample('15min').ffill()\n",
    "\n",
    "# Reset index after resampling to bring 'date_time' back as a column\n",
    "weather_df = weather_df.reset_index()\n",
    "\n",
    "# Check the result\n",
    "print(weather_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36a0680-922f-4db8-abbc-f4e919928613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the nearest stations data with weather zone info\n",
    "nearest_stations_with_zones = nearest_stations.merge(weather_df[['weather_source_id', 'weather_zone', 'zone_label']], \n",
    "                                                     left_on='Nearest_Lon', right_on='lon', how='left')\n",
    "\n",
    "# Display the nearest stations with their respective weather zones\n",
    "print(nearest_stations_with_zones[['x', 'y', 'Nearest_Lon', 'Nearest_Lat', 'zone_label']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c2aaa2-4adb-4222-ac73-6fb6912b58f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fine-Tune the Zones (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01230c3b-7466-4e65-a1af-1d577958e251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import googlemaps\n",
    "import pandas as pd\n",
    "\n",
    "# Set up Google Maps API client\n",
    "api_key = 'YOUR_GOOGLE_API_KEY'  # Replace with your actual API key\n",
    "gmaps = googlemaps.Client(key=api_key)\n",
    "\n",
    "# Function to get the elevation of a coordinate using Google Maps Elevation API\n",
    "def get_elevation(lat, lon):\n",
    "    result = gmaps.elevation((lat, lon))\n",
    "    return result[0]['elevation'] if result else None\n",
    "\n",
    "# Example: Apply to your weather_df\n",
    "weather_df['elevation'] = weather_df.apply(lambda row: get_elevation(row['lat'], row['lon']), axis=1)\n",
    "\n",
    "# View the stations with their elevation data\n",
    "print(weather_df[['weather_source_id', 'lat', 'lon', 'elevation']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f335d2-278b-4a58-b8c5-bdc9a5ee841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a few geohashes to check if they are unique\n",
    "print(consumption_df[['name', 'geohash']].head())\n",
    "print(weather_df[['name', 'geohash']].head())\n",
    "\n",
    "# Check if geohashes are unique\n",
    "print(consumption_df['geohash'].nunique())\n",
    "print(weather_df['geohash'].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8b0523-ae98-47d0-922c-396f197a7d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aca3d2e-33d3-4242-9d50-77780daa60db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygeohash\n",
    "print(dir(pygeohash))  # This will show all the available functions in the module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b37cf5-bd23-46a5-9791-b233963e3e7b",
   "metadata": {},
   "source": [
    "## Keep necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8095fc-408e-4967-b2dc-16bd24e40f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f933facc-e72f-46a8-bb6c-259bc13d1799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to keep\n",
    "columns_to_keep = [\n",
    "    'date time', 'ABDOON_P', 'ALHIZAM_P', 'ALSALT_P', 'AQABAIN_P',\n",
    "    'ASHRFIA_P', 'ATTARATX_P', 'AZRAQ_P', 'BROADCA_P', 'C_CENTER_P',\n",
    "    'DHULEIL_P', 'DISI_P', 'EL_HASA_P', 'HASHMYA_P', 'HASAN_INDU_P',\n",
    "    'HUSAIN_UN_P', 'IRBID_E P', 'IRBID_N P', 'ISHTAFIN P', 'KARAK P',\n",
    "    'MADABA_S P', 'MAFRAQ_P', 'MANARAH_P', 'MARQA_P', 'M_CEMENT P',\n",
    "    'N_KARAK P', 'QATRANA P', 'QUWEIRA P', 'Q_CEMENT P', 'RAJIHI P',\n",
    "    'RAMA P', 'RASHADI_P', 'REHAB P', 'RESHA_N P', 'REWESHED P', 'SABHA P',\n",
    "    'SAFAWI P', 'SAHAB P', 'SHEDIA_P', 'SUBEIHI P', 'SWEIMEH P',\n",
    "    'TAFILA_C P', 'TAREQ P', 'UNIVRSTY P', 'WAQAS P', 'ABDALI_P',\n",
    "    'AMMANS_P', 'AQABA A2_P', 'AQABA_TH_P', 'BAYADER_P', 'GHORSAF_P',\n",
    "    'MAAN_P', 'Q.A.I.A_P', 'ZERQA_P', 'id', 'name', 'lon', 'lat', 'alt',\n",
    "    'wind_speed_knot', 'wind_direction_degree', 'irradiance',\n",
    "    'relative_humidity_percent', 'air_temperature_c', 'rainfall_mm',\n",
    "    'irradiance_corrected'\n",
    "]\n",
    "\n",
    "# Keep only the specified columns\n",
    "merged_df = merged_df[columns_to_keep]\n",
    "\n",
    "# Check the cleaned columns\n",
    "print(\"Filtered Merged Data Columns:\", merged_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1439ee05-338c-4809-88c2-2776d76fad58",
   "metadata": {},
   "source": [
    "## Delete empty columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e14ebe-caa3-44c3-857e-e3f0b650fd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing values in each column\n",
    "missing_percentage = merged_df.isnull().mean() * 100\n",
    "\n",
    "# Filter for columns that have missing values (greater than 0%)\n",
    "columns_with_missing_values = missing_percentage[missing_percentage > 0]\n",
    "\n",
    "# Display columns with missing values and their percentages\n",
    "print(\"Columns with Missing Values and Their Percentages:\")\n",
    "print(columns_with_missing_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8913d63a-faf6-4037-9590-76d499a1e60e",
   "metadata": {},
   "source": [
    "## finding better imputation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bd432b-9f35-4e53-a665-c4574e036959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data types for the columns with missing values\n",
    "missing_columns = [\n",
    "    'wind_speed_knot', 'wind_direction_degree', 'irradiance',\n",
    "    'relative_humidity_percent', 'air_temperature_c', 'rainfall_mm'\n",
    "]\n",
    "\n",
    "data_types_missing = merged_df[missing_columns].dtypes\n",
    "print(\"Data Types for Columns with Missing Values:\")\n",
    "print(data_types_missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ebe7c6-9996-4076-876c-fb602eee706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(merged_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8c8353-65a1-49a4-a00b-fe1ea66efa4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Ensure merged_df is a DataFrame (already confirmed)\n",
    "# Select numeric columns for KNN imputation\n",
    "numeric_columns = merged_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Initialize KNN Imputer\n",
    "knn_imputer = KNNImputer(n_neighbors=3)  # You can adjust the number of neighbors\n",
    "\n",
    "# Perform KNN imputation\n",
    "knn_imputed_data = knn_imputer.fit_transform(merged_df[numeric_columns])\n",
    "knn_imputed_df = pd.DataFrame(knn_imputed_data, columns=numeric_columns)\n",
    "\n",
    "# Replace original numeric columns with imputed values\n",
    "merged_df[numeric_columns] = knn_imputed_df\n",
    "\n",
    "# Check the result\n",
    "print(merged_df.isnull().sum())  # Print the count of missing values in each column after imputation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ad6933-7ba8-475b-ad59-d6a36f13fe2b",
   "metadata": {},
   "source": [
    "## Fill the nulls with median for the columns that still have nulls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb75b7f-174c-498c-9a7b-5698ed32cd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to fill with median values\n",
    "columns_to_fill = merged_df.columns[merged_df.isnull().any()]\n",
    "\n",
    "# Fill missing values with the median for the specified columns\n",
    "for column in columns_to_fill:\n",
    "    median_value = merged_df[column].median()\n",
    "    merged_df[column].fillna(median_value, inplace=True)\n",
    "\n",
    "# Check if there are still missing values after filling\n",
    "missing_values_count = merged_df.isnull().sum()\n",
    "if (missing_values_count > 0).any():\n",
    "    print(\"Some columns still have missing values:\")\n",
    "    print(missing_values_count[missing_values_count > 0])\n",
    "else:\n",
    "    print(\"All missing values have been filled with median values.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65699171-dabb-4376-a456-2a9d69eeb16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cff5f6-dec6-4029-a4e8-27a5f72fc894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce300c9-63f8-48f0-ba05-71d3cf858d32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e748b75-554f-4d58-bc97-66e23d5eb262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410eff38-36ba-4e56-b968-2431091987f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f2a50f-e2ec-404b-a65e-6a476e5451cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44219d8-ede7-4f86-9345-d4dadb56bbb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base]",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
